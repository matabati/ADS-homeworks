# Which Machine Learning Models Are Actually Used in Industry?


## Executive summary

Across most industries, **classical “tabular-data” models still dominate production ML**, especially **linear/logistic regression** and **tree-based ensembles** (Random Forests + Gradient-Boosted Decision Trees/GBDT such as XGBoost/LightGBM/CatBoost). In Kaggle’s 2021 industry-focused survey report of working data scientists, **~80% report using linear/logistic regression** and **~74% report decision trees/random forests**, while **~60% report GBDT** usage—substantially higher than any single deep learning family in that snapshot. 
Deep learning is **highly prevalent where unstructured data or ranking at scale matters** (ads, recommendations, search, vision, speech, and modern NLP). Large consumer tech firms commonly report transformer adoption for personalization and ranking systems, and deep learning’s operational footprint can be enormous (e.g., Uber reports **5K+ models in production** and notes a transition from “simple tree models” to deep learning and then to generative AI in its platform evolution). 
Over the last 2–3 years, **LLMs/foundation-model usage has moved from pilot to early production**, but with material constraints (cost, latency, governance, and fit-for-purpose). O’Reilly’s 2023 enterprise survey reporting (as summarized by InfoWorld) found **about two-thirds of respondents using generative AI**, but only **~18% reporting applications in production**—a telling “interest vs. operationalization” gap. 


## Methodology and prioritized sources

This review triangulates “what is used in industry” using four complementary evidence types:

First, **large practitioner surveys** are used to approximate prevalence and tooling, prioritized for direct quantitative readouts. The anchor is Kaggle’s *State of Machine Learning & Data Science 2021* report (PDF: `https://storage.googleapis.com/kaggle-media/surveys/Kaggle%27s%20State%20of%20Machine%20Learning%20and%20Data%20Science%202021.pdf`), which reports algorithm and framework usage percentages among working data scientists. We supplement with Kaggle’s 2022 survey slide deck PDF (official: `https://storage.googleapis.com/kaggle-media/surveys/Kaggle%20State%20of%20Machine%20Learning%20and%20Data%20Science%20Report%202022.pdf`) for directional trend signals, including rising transformer usage for deep learning. 
Second, **developer ecosystem signals** help validate what stacks teams actually build with. Stack Overflow’s 2024 survey shows widespread adoption of AI tooling in development workflows (and rapid growth year-over-year), which is a strong proxy for the diffusion of LLM-enabled product work and internal tooling. 
Third, **industry/vendor operational telemetry and reports** are used for production intensity and organizational adoption trends, notably Databricks’ reporting based on aggregated customer usage (e.g., rapid growth in production deployments and vector DB usage for RAG-style systems). Source: `https://www.databricks.com/blog/state-ai-enterprise-adoption-growth-trends`. 
Fourth, **primary company engineering blogs and major papers** provide concrete, cited examples of models used in real systems, plus context on why. Examples used here include Uber (XGBoost and large-scale MLOps), Netflix (transformer foundation models in personalization), Google (Wide & Deep in production), Meta (DLRM-based ad recommendations), Microsoft (anomaly detection service algorithms), and Google Research/NeurIPS (applied RL for data-center control).
Limitations: survey results capture *self-reported usage* (often “used regularly”) rather than the fraction of *production inference traffic*; company tech blogs overrepresent sophisticated organizations and published successes; and “industry” spans everything from low-data, regulation-heavy settings to hyperscale consumer systems.

## Current prevalence of model families in industry

The most consistent pattern across surveys and production case studies is a **two-regime reality**:
**Regime one: tabular business ML** (risk, churn, propensity, pricing, forecasting features, operations). This regime disproportionately uses **linear models and tree ensembles**, because they’re fast, robust with modest data, easy to deploy, and compatible with governance and explanation needs. Kaggle’s 2021 report quantifies this directly: linear/logistic regression (80.3%), decision trees/random forests (74.1%), and gradient boosting machines (59.5%) top the list of commonly used methods.
**Regime two: unstructured + ranking + language** (ads, recommendations, search relevance, vision, speech, and modern NLP). This regime uses deep learning heavily—CNNs for vision, transformers for text and increasingly for multimodal and recommendation—and is where model scale and inference throughput become defining constraints. For example, Netflix describes deploying a large **transformer-based foundation model** for personalization, and explicitly states multiple integration approaches are **“now used in production for different use cases.”
### Relative prevalence snapshot with a simple chart
Below is an **indicative prevalence snapshot** using Kaggle 2021 “methods & algorithms usage” percentages (share of surveyed working data scientists reporting use; multi-select question, so totals exceed 100%). Source: Kaggle 2021 PDF.
```
Kaggle 2021 (working data scientists): % using the method

Linear/Logistic Regression         80% | ████████████████████████████████████████
Decision Trees / Random Forests    74% | ██████████████████████████████████████
GBDT (XGBoost/LightGBM/etc.)       60% | ████████████████████████████████
Convolutional Neural Nets (CNNs)   40% | ████████████████████
Bayesian approaches                29% | ███████████████
Dense NNs (MLPs)                   28% | ███████████████
Recurrent NNs (RNNs)               27% | ██████████████
Transformer networks (BERT/GPT-3)  17% | █████████
GANs                                 8% | ████
```

Two important trend notes:

Kaggle’s 2021 report already observed growth in “large language models such as transformer networks,” and its 2022 deck shows **transformer architectures rising strongly (roughly low-40s% to ~60%+) among deep learning users over 2019–2022**. 
Tooling follows the same shape: in Kaggle 2021, **scikit-learn** remains the most used ML framework (82.3%), with **TensorFlow/Keras** around half, **XGBoost** and **PyTorch** substantial, and **Hugging Face** and **Prophet** around ~10% each—evidence that classical ML + boosting + emergent transformer tooling coexist in working practice. 

## Model families compared

The table below summarizes **what’s used**, **where**, and **why**. “Adoption” is qualitative (High/Medium/Low) grounded in the cited surveys and production examples above; where the table infers beyond direct survey percentages, it is stated as an estimate.

| Model family | Industry adoption today | Typical domains/data | Strengths | Weaknesses | Data & compute needs | Interpretability | Typical tooling |
|---|---|---|---|---|---|---|---|
| Linear regression / GLMs | **High** (especially tabular)| Pricing, risk, demand features, simple forecasting baselines | Very fast; stable; strong baseline; easy to monitor | Limited nonlinear interactions unless engineered | Low | High | scikit-learn (`https://github.com/scikit-learn/scikit-learn`)  |
| Logistic regression | **High**; especially regulated scoring  | Credit scoring, fraud/risk, medical risk models, propensity | Strong calibration baseline; governance-friendly | May underfit complex interactions | Low–Medium | High | scikit-learn LogisticRegression docs (`https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html`)|
| Decision Trees / Random Forests | **High** (trees very common; RF common) | Tabular classification/regression; feature importance | Robust; handles nonlinearities; RF reduces variance | RF can be heavy; less calibrated; less transparent than linear | Low–Medium | Medium | scikit-learn RandomForestClassifier docs (`https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html`) |
| GBDT (XGBoost / LightGBM / CatBoost) | **High**; backbone for many tabular + ranking systems | Fraud/risk, ranking, churn, pricing, ops, ads features | Strong on tabular; good accuracy/latency tradeoff; mature tooling | Careful tuning; can overfit leakage; explanations are surrogate | Medium | Medium | XGBoost (`https://github.com/dmlc/xgboost`); LightGBM (`https://github.com/microsoft/LightGBM`); CatBoost (`https://github.com/catboost/catboost`) |
| SVMs | **Lower / niche** (estimate; less emphasized in top-line industry survey summaries) | Small/medium datasets; some text or high-dim | Strong margins; good with defined kernels | Scaling, tuning, feature engineering; less flexible than ensembles | Medium | Medium | scikit-learn SVM guide (`https://scikit-learn.org/stable/modules/svm.html` |
| KNN | **Lower / niche** | Similarity search, small-data classification; retrieval baselines | Simple; no training; local adaptivity | Slow at inference at scale; sensitive to scaling/metrics | Low training / High inference | Low | scikit-learn KNeighborsClassifier docs (`https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html`) |
| Naive Bayes | **Lower–Medium** (common baseline; strong in some text problems) | Fast text classification; spam baselines | Extremely fast; works well with conditional independence-ish features | Assumptions often false; may underperform better models | Low | Medium | scikit-learn Naive Bayes guide (`https://scikit-learn.org/stable/modules/naive_bayes.html`) |
| Classical time-series (ARIMA/ETS, decomposable models) | **High in forecasting orgs**, often alongside ML | Demand/finance forecasting; capacity planning | Transparent; strong with seasonality & good feature design | Limited with many related series; manual tuning sometimes | Low | Medium–High | statsmodels (`https://github.com/statsmodels/statsmodels`); Prophet (`https://github.com/facebook/prophet`) |
| Neural time-series (RNN/LSTM; DeepAR) | **Medium and growing** in scale-forecasting stacks | Multi-series forecasting; retail demand | Learns across many series; probabilistic forecasts | Needs more data; monitoring/ops harder | Medium–High | Low–Medium | AWS DeepAR docs (`https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html`) |
| CNNs | **Medium** overall; **high** in vision-heavy domains | Medical imaging, OCR, inspection, perception | Excellent for images; mature deployment toolchains | Needs labeled data; can be brittle to shift | High | Low | PyTorch (`https://github.com/pytorch/pytorch`); TensorFlow (`https://github.com/tensorflow/tensorflow`) |
| Transformers (non-LLM, incl. BERT-style encoders) | **Medium–High** in NLP/ranking; rising fast | NLP, search/recs, multimodal | Strong representations; pretraining leverage | Cost/latency; complexity in serving | High | Low | Hugging Face Transformers (`https://github.com/huggingface/transformers`); “Attention Is All You Need” (`https://arxiv.org/abs/1706.03762`) |
| LLMs / foundation models | **Rapidly rising**; productionization still uneven | Copilots, support, search, personalization, content ops | Strong zero-/few-shot; natural language interface; accelerates dev | Cost, latency, privacy/IP, evaluation difficulty | Very high (esp. training) | Low | HF Transformers; enterprise telemetry (Databricks)  |
| Probabilistic/Bayesian modeling | **Medium in specific domains**; not universal | Experimentation, causal, risk, forecasting uncertainty | Principled uncertainty; credible intervals | Compute + expertise; slower inference | Medium | Medium | PyMC (`https://github.com/pymc-devs/pymc`); Stan/CmdStan (`https://github.com/stan-dev/cmdstan`) |
| Reinforcement learning | **Low overall**, used where control/optimization is core | Robotics, ops control, resource allocation | Optimizes sequential decisions | Hard deployment; safety and evaluation risks | High | Low | Google Research: RL “in the wild” for data centers (`https://research.google/pubs/data-center-cooling-using-model-predictive-control/`) ; Ray/RLlib (`https://docs.ray.io/en/latest/rllib/index.html`) |

## Industry use cases and cited real-company examples

In finance and regulated risk, **logistic regression scorecards remain a default** because they are easier to implement and explain, and align with governance expectations. A credit-scorecard industry write-up describes “a standard credit scorecard model” as being based on logistic regression, emphasizing explainability and ease of implementation.  Academic/industry discussion similarly notes logistic regression remains a benchmark in credit risk largely due to interpretability/regulatory constraints, even when ensembles can outperform it. 

In large-scale marketplaces and consumer tech, **GBDT and deep learning coexist**, often with GBDT as an initial “workhorse” and deep learning introduced where it yields incremental ranking/relevance gains. Airbnb’s published account of search ranking reports that “much of the initial gains were driven by a gradient boosted decision tree model,” and then documents the push toward neural approaches to surpass performance plateaus. Uber describes its early phase as primarily “predictive” ML for tabular use cases (explicitly citing XGBoost for ETA, risk assessment, pricing), and later phases that moved more tier-1 models to deep learning and then to generative AI. 

In advertising and recommendation, modern stacks are frequently **hybrid**: wide (linear) components for memorization plus deep networks for generalization, or deep recommendation models with large embedding tables. Google’s “Wide & Deep” paper explicitly states the system was **productionized and evaluated on Google Play** (a commercial-scale recommender). Meta describes its ad recommendation engine as being powered by **DLRM-based recommendation models**, highlighting deep learning’s centrality for personalized ads at scale.

In personalization and content discovery, **transformer-based foundation models are now being operationalized** beyond text-only chat interfaces. Netflix reports integrating a **transformer-based foundation model** into personalization applications via multiple engineering patterns (embeddings, subgraph integration, and fine-tuning) and explicitly says these approaches are “now used in production for different use cases.” 

In healthcare (especially imaging), deep learning is prominent because the data modality is visual and outcomes often depend on subtle patterns in pixels. The FDA publishes an “AI-Enabled Medical Device List” of authorized devices, indicating a large and growing regulated footprint for ML-enabled clinical software. Reporting on FDA list updates highlights radiology as a major share of authorized AI devices, consistent with imaging-centric model usage in that space. 

In time-series forecasting and anomaly detection, industry uses a mix of **classical statistical models, decomposable models, and neural approaches**. Prophet (originated at Facebook/Meta) is positioned as a practical forecasting procedure for seasonality and trend, and remains widely used as a business-forecasting tool. AWS’s DeepAR algorithm documentation describes DeepAR as an RNN-based approach for forecasting many related time series and contrasts it with ARIMA/ETS “one model per series” classical approaches. For anomaly detection, Microsoft described a production time-series anomaly detection service using a Spectral Residual + CNN approach (SR-CNN), explicitly tied to Microsoft production data. Databricks provides an applied pattern using scikit-learn’s Isolation Forest in a near-real-time pipeline, reflecting common “classical unsupervised ML + MLOps” practice.

Reinforcement learning is comparatively rare, but does see credible deployments in control settings. A NeurIPS paper and Google Research entry describe **model-based reinforcement learning applied “in the wild”** to data-center cooling control via model-predictive control. 

## Barriers to adoption and measurable signals of what’s really being used

The biggest barrier differentiator between “classic ML” and “deep/LLM” stacks is not conceptual—it’s operational: **data readiness, compute cost, latency, governance, and reliability requirements**.

Generative AI/LLM barriers show up strongly in enterprise surveys: InfoWorld’s summary of O’Reilly’s enterprise report highlights **difficulty finding business use cases, legal uncertainties, and high infrastructure costs** as top issues holding adoption back; it also reports that many adopters are still early, with only a minority having apps in production. This aligns with vendor telemetry: Databricks reports rapid growth in production models and infrastructure for RAG (vector DB growth), implying organizations are building the plumbing—but still sorting out standardization and ROI. 

Signals that help quantify “what’s used,” even when model choices aren’t disclosed publicly, include:

GitHub stars (tool adoption proxy): TensorFlow (~194k), PyTorch (~97k), scikit-learn (~65k), Hugging Face Transformers (~156k), XGBoost (~28k), LightGBM (~18k), Prophet (~20k) provide a rough sense of the mainstream tooling surface area developers invest in.

Job posting and labor-market signals: Stanford HAI’s 2024 AI Index reports AI-related job postings as a measurable share of postings (e.g., it cites a decline from 2.0% to 1.6% of all U.S. job postings from 2022 to 2023), illustrating both how adoption is tracked and how cyclical hiring can be even while tooling grows. Meanwhile, Indeed-based reporting indicates strong growth in postings mentioning AI over longer windows (a different “AI mentions” lens), suggesting sustained demand for AI-capable builders beyond pure “data scientist” roles. 

Production case-study intensity: Uber’s platform metrics—**~5K models in production** and **~10M real-time predictions per second at peak**—illustrate that in mature orgs, the key question becomes not “which single model?” but “how many models, how often retrained, and how well governed?”.
## Conclusion and forecast for model usage over the next 5–10 years

Classical models (linear/logistic regression and GBDT) are likely to **remain the modal choice for the median enterprise prediction problem** over the next decade, because the median enterprise problem remains tabular, cost-sensitive, and governance-heavy. The strongest evidence is the persistent dominance of these methods in working-practitioner surveys (Kaggle 2021) and the repeated pattern in company histories: Uber explicitly describes early phases dominated by XGBoost for core tabular tasks and even notes that “in several cases, XGBoost outperforms [deep learning] in both performance and cost.”  In regulated domains like credit risk, interpretability and compliance pressures will continue to favor GLMs/scorecards and constrained ensembles. 

Deep learning—especially transformers—will still **gain share in the subset of domains where it is structurally advantaged**: recommendations/ads/search ranking, multimodal personalization, and language-centric workflows. Kaggle’s 2022 survey deck shows rapid growth of transformer adoption among deep learning users, and Netflix’s 2025 account makes clear that large transformer-based personalization foundation models are now being integrated into production using patterns (embedding stores, subgraph integration, fine-tuning) that will likely become industry templates.  Vendor telemetry suggests the “LLM application stack” (vector DBs/RAG, model catalogs, deployment/monitoring) is scaling quickly in enterprises, which should lower the barrier to deploying specialized foundation-model components even when the final “decision model” remains a GBDT or logistic layer. 
LLMs will expand most through **workflow embedding (copilots, agents, retrieval + ranking, and feature generation)** rather than by fully replacing classical predictors everywhere. Survey evidence suggests enthusiasm outpaces production maturity (two-thirds using genAI but a much smaller fraction with production apps), and enterprise constraints (legal risk, cost, unclear use cases) will keep many deployments hybrid and tightly scoped. Over 5–10 years, expect “industry model usage” to look less like a single winner and more like a **layered architecture**: classical models dominating tabular decision cores; deep learning dominating representation learning and ranking; and LLMs increasingly serving as the interface and the glue between data, tools, and users—especially as organizations standardize evaluation, monitoring, and governance for generative systems.
